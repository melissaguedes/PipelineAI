# -*- coding: utf-8 -*-
"""Desafio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BPBTaotNlB6AL570uBKyZ9p_xvhemw2A

**MARK:** Preparação do ambiente e importações de dependências
"""

!pip install pytesseract pdfplumber sentence-transformers faiss-cpu

!apt-get update
!apt-get install -y tesseract-ocr poppler-utils

!apt-get install -y tesseract-ocr-por

!pip install opencv-python-headless

!pip install --upgrade google-generativeai

!tesseract --version

"""**MARK:** Upload dos arquivos"""

from google.colab import files
uploaded = files.upload()

"""**MARK:** Script 1

Este script percorre os arquivos enviados (PDFs e imagens), aplica técnicas de extração de texto:

- Utiliza `pdfplumber` para extrair texto de PDFs com texto nativo.
- Utiliza `pytesseract` para aplicar OCR em imagens e PDFs digitalizados.
- Salva o conteúdo extraído em um único arquivo chamado `texto_extraido.txt`.

Este é o primeiro passo da pipeline e prepara os dados para a fase de  indexação.
"""

import pytesseract
import pdfplumber
from PIL import Image
import os

def extract_text_from_pdf(path):
    try:
        with pdfplumber.open(path) as pdf:
            texts = []
            for page in pdf.pages:
                text = page.extract_text()
                if not text or len(text.strip()) < 30:
                    print(f"[OCR] Página com conteúdo fraco, aplicando OCR em {path}")
                    image = page.to_image(resolution=300).original
                    ocr_text = pytesseract.image_to_string(image, lang='por')
                    texts.append(ocr_text)
                else:
                    texts.append(text)
            return "\n".join(texts)
    except Exception as e:
        print(f"[ERROR] Failed to process PDF: {path}", e)
        return ""

def extract_text_from_image(path):
    try:
        import cv2
        import numpy as np

        image = cv2.imread(path)

        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

        scale_percent = 150
        width = int(thresh.shape[1] * scale_percent / 100)
        height = int(thresh.shape[0] * scale_percent / 100)
        resized = cv2.resize(thresh, (width, height), interpolation=cv2.INTER_LINEAR)

        from PIL import Image
        pil_img = Image.fromarray(resized)
        text = pytesseract.image_to_string(pil_img, lang="por")
        return text
    except Exception as e:
        print(f"[ERROR] Failed to process image: {path}", e)
        return ""

def extract_all_text(uploaded_files):
    full_text = ""
    for name, content in uploaded_files.items():
        with open(name, "wb") as f:
            f.write(content)
        if name.lower().endswith(".pdf"):
            text = extract_text_from_pdf(name)
        elif name.lower().endswith((".png", ".jpg", ".jpeg", ".webp")):
            text = extract_text_from_image(name)
        else:
            continue
        full_text += f"\n--- {name} ---\n{text}\n"
    return full_text

extracted_text = extract_all_text(uploaded)

with open("extracted_text.txt", "w", encoding="utf-8") as f:
    f.write(extracted_text)

print("Text extraction completed!")

"""**MARK:** Script 2

Este script divide o texto extraído em pequenos trechos (chunks), gera embeddings vetoriais e os armazena em um índice vetorial FAISS:

- Utiliza o modelo `all-MiniLM-L6-v2` da `sentence-transformers` para criar vetores semânticos.
- Armazena os vetores no FAISS para busca eficiente.
- Salva os chunks em `chunks.txt` e o índice vetorial em `index.faiss`.

É a etapa responsável por tornar o conteúdo pesquisável de forma semântica.
"""

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

def chunk_text(path, chunk_size=800):
    with open(path, "r", encoding="utf-8") as f:
        text = f.read()
    words = text.split()
    return [" ".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]

chunks = chunk_text("extracted_text.txt")
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embedding_model.encode(chunks)

index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(np.array(embeddings))

print("Indexing completed!")

"""**MARK:** Script 3

Este script permite ao usuário digitar perguntas em linguagem natural. O sistema:

1. Converte a pergunta em um vetor semântico.
2. Busca os trechos mais relevantes no índice FAISS.
3. Envia o contexto encontrado para o modelo Gemini da Google.
4. Retorna uma resposta precisa com base nos documentos carregados.

A LLM utilizada é o modelo `gemini-2.5-flash`, configurado com sua chave da API.
"""

import google.generativeai as genai
import os
os.environ["GEMINI_API_KEY"] = "AIzaSyDtvouGZSTcpXcMhigxtOBkpshh8eyN1Cc"

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
if not GEMINI_API_KEY:
    raise ValueError("API Key ausente! Configure GEMINI_API_KEY.")

genai.configure(api_key=GEMINI_API_KEY)
llm = genai.GenerativeModel(model_name="models/gemini-2.5-flash")

def retrieve_chunks(question, k=3):
    question_vec = embedding_model.encode([question])
    _, indices = index.search(np.array(question_vec), k)
    return [chunks[i] for i in indices[0]]

def generate_answer(question):
    context = "\n\n".join(retrieve_chunks(question))
    prompt = f"Com base no conteúdo abaixo, responda à pergunta:\n\n{context}\n\nPergunta: {question}"
    response = llm.generate_content(prompt)
    return response.text

print("Faça perguntas com base nos documentos (digite 'sair' para encerrar)")
while True:
    q = input("Pergunta: ")
    if q.lower() in ["sair", "exit", "quit"]:
        print("Sessão encerrada.")
        break
    answer = generate_answer(q)
    print(f"\nResposta:\n{answer}\n")